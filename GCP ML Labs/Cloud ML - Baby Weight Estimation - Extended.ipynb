{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML with Structured Data using Google Cloud\n",
    "\n",
    "This tutorial is adapted from [this awesome tutorial](https://docs.google.com/presentation/d/e/2PACX-1vR-d6ztE9pkRS1L0pKInaaGMsBf7d_bMETr3Mx0uFYng2Y22zexg0ZaPRWbmmc497EMBeRgg8xvLLfI/pub?start=false&loop=false&delayms=3000&slide=id.g3444070087_0_0) created by **Lak Lakshmanan** for end-to-end ML with TensorFlow on GCP, which includes the original [codelabs](https://codelabs.developers.google.com/codelabs/end-to-end-ml/#0). It extends on original one by covering: Facets, BQML, TFT, and TFMA.\n",
    "\n",
    "This notebook illustrates:\n",
    "\n",
    "1. Exploring a BigQuery dataset using Datalab & Facets\n",
    "2. Linear Regression with BQML\n",
    "3. Creating datasets for Machine Learning using Dataflow & tf.Transform\n",
    "4. Creating a model using the high-level Estimator API \n",
    "5. Evaluate model quality using TensorFlow Model Analysis\n",
    "5. Training & Tuning using Cloud ML Engine\n",
    "5. Deploying the model\n",
    "6. Predicting with the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housekeeping "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET = 'ksalama-gcs-cloudml'\n",
    "PROJECT = 'ksalama-gcp-playground'\n",
    "REGION = 'europe-west1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_data_dir = 'gs://{0}/data/babyweight/'.format(BUCKET)\n",
    "gcs_model_dir = 'gs://{0}/models/babyweight/'.format(BUCKET)\n",
    "\n",
    "local_data_dir = 'data/babyweight'\n",
    "local_models_dir= 'models/babyweight'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m rm -rf gs://ksalama-gcs-cloudml/models/babyweight/*\n",
    "gsutil -m rm -rf gs://ksalama-gcs-cloudml/data/babyweight/big_data/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Exploring data in BigQuery\n",
    "\n",
    "The data is natality data (record of births in the US). My goal is to predict the baby's weight given a number of factors about the pregnancy and the baby's mother.  Later, we will want to split the data into training and eval datasets. The hash of the year-month will be used for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bq query --name data\n",
    "\n",
    "SELECT\n",
    "  CAST(mother_race AS string) race_index,\n",
    "  AVG(weight_pounds) avg_weight,\n",
    "  COUNT(weight_pounds) instance_Count\n",
    "FROM\n",
    "  `publicdata.samples.natality`\n",
    "WHERE \n",
    "    year > 2000\n",
    "AND weight_pounds > 0\n",
    "AND mother_age > 0\n",
    "AND plurality > 0\n",
    "AND gestation_weeks > 0\n",
    "AND month > 0\n",
    "AND mother_race is not null\n",
    "GROUP BY\n",
    "  mother_race\n",
    "ORDER BY\n",
    "  avg_weight DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualise with Datalab commands \n",
    "http://googledatalab.github.io/pydatalab/google.datalab%20Commands.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%chart columns --data data --fields race_index,avg_weight\n",
    "title: Mother Race Index vs Average Baby Weight\n",
    "height: 400\n",
    "width: 900\n",
    "hAxis:\n",
    "  title: Race Index\n",
    "vAxis:\n",
    "  title: Average Weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data from BigQuery as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%sql --module query \n",
    "\n",
    "SELECT\n",
    "  ROUND(weight_pounds,1) AS weight_pounds,\n",
    "  is_male,\n",
    "  mother_age,\n",
    "  mother_race,\n",
    "  plurality,\n",
    "  gestation_weeks,\n",
    "  mother_married,\n",
    "  cigarette_use,\n",
    "  alcohol_use\n",
    "FROM\n",
    "  `publicdata.samples.natality`\n",
    "WHERE \n",
    "        year > 2000\n",
    "    AND weight_pounds > 0\n",
    "    AND mother_age > 0\n",
    "    AND plurality > 0\n",
    "    AND gestation_weeks > 0\n",
    "    AND month > 0\n",
    "    AND mother_race IS NOT NULL\n",
    "LIMIT $DATA_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datalab.bigquery as bq\n",
    "import sys\n",
    "data = bq.Query(query, DATA_SIZE = data_size).to_dataframe(dialect='standard')\n",
    "print('Row count:{}'.format(len(data)))\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore & Visualise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "#plt.figure(figsize=(45, 25))\n",
    "plt.figure(figsize=(30, 15))\n",
    "\n",
    "# Baby Weight Distribution\n",
    "plt.subplot(2,3,1)\n",
    "plt.title(\"Baby Weight Histogram\")\n",
    "plt.hist(data.weight_pounds, bins=150)\n",
    "#plt.axis([0, 50, 0, 3500])\n",
    "plt.xlabel(\"Baby Weight Ranges\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "# ---------------------------\n",
    "\n",
    "# Mother Age vs Baby Weight\n",
    "plt.subplot(2,3,2)\n",
    "plt.title(\"Mother Age vs Baby Weight\")\n",
    "plt.scatter(data.mother_age,data.weight_pounds)\n",
    "plt.xlabel(\"Mother Age\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "# ---------------------------\n",
    "\n",
    "# Gestation Weeks vs Baby Weight\n",
    "plt.subplot(2,3,3)\n",
    "fit = np.polyfit(data.gestation_weeks,data.weight_pounds, deg=1)\n",
    "plt.plot(data.gestation_weeks, fit[0] * data.gestation_weeks + fit[1], color='red')\n",
    "plt.scatter(data.gestation_weeks, data.weight_pounds)\n",
    "plt.xlabel(\"Gestation Weeks\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "\n",
    "#---------------------------\n",
    "\n",
    "# Is Male vs Baby Weight Boxplot\n",
    "plt.subplot(2,3,4)\n",
    "plt.title(\"Is Male vs Baby Weight\")\n",
    "\n",
    "is_male_values = list(data.is_male.value_counts().index.values)\n",
    "is_male_data = []\n",
    "for i in is_male_values:\n",
    "    is_male_data = is_male_data + [data.weight_pounds[data.is_male == i].values]\n",
    "\n",
    "plt.boxplot(is_male_data)\n",
    "plt.axis([0, 3, 4, 11])\n",
    "plt.xlabel(\"Is Male\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "# ---------------------------\n",
    "\n",
    "# Mother Race vs Baby Weight Boxplot\n",
    "plt.subplot(2,3,5)\n",
    "plt.title(\"Mother Race vs Baby Weight\")\n",
    "\n",
    "race_values = list(data.mother_race.value_counts().index.values)\n",
    "race_data = []\n",
    "for i in race_values:\n",
    "    race_data = race_data + [data.weight_pounds[data.mother_race == i].values]\n",
    "\n",
    "plt.boxplot(race_data)\n",
    "plt.axis([0, 16, 4, 11])\n",
    "plt.xlabel(\"Mother Race\")\n",
    "plt.ylabel(\"Baby Weight\")\n",
    "\n",
    "# # ---------------------------\n",
    "\n",
    "plt.subplot(2,3,6)\n",
    "plt.title(\"Alcohol & Cigarette Use\")\n",
    "\n",
    "alch_use_values = list(data.alcohol_use.value_counts().index.values)\n",
    "cig_use_values = list(data.cigarette_use.value_counts().index.values)\n",
    "\n",
    "use_data = []\n",
    "labels = []\n",
    "\n",
    "for i in alch_use_values:\n",
    "    for j in cig_use_values:\n",
    "        labels = labels + ['alch-use:{} & cig-use:{}'.format(i,j)]\n",
    "        condition = (data.alcohol_use == i) & (data.cigarette_use == j)\n",
    "        values = data.weight_pounds[condition].values\n",
    "        if (len(values) > 0):\n",
    "            use_data = use_data + [len(values)]\n",
    "\n",
    "plt.pie(use_data)\n",
    "plt.legend(labels, loc=\"lower center\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise Dataset using Facets - Big Picture\n",
    "visit: https://research.google.com/bigpicture/\n",
    "\n",
    "* Use Stacked with categorical features to test the distribution  If used with numerical features will bucketise them.\n",
    "* Use Scatter with numerical values to test correlations.\n",
    "* Use Facets to slice and dice (vertically and horizontally).\n",
    "* Use colour with the target feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "\n",
    "jsonstr = data.to_json(orient='records')\n",
    "\n",
    "#HTML_TEMPLATE = \"\"\"<link rel=\"import\" href=\"/nbextensions/facets-dist/facets-jupyter.html\">\n",
    "HTML_TEMPLATE = \"\"\"<link rel=\"import\" href=\"/nbextensions/facets-jupyter.html\">\n",
    "        <facets-dive id=\"elem\" height=\"600\"></facets-dive>\n",
    "        <script>\n",
    "          var data = {jsonstr};\n",
    "          document.querySelector(\"#elem\").data = data;\n",
    "        </script>\"\"\"\n",
    "html = HTML_TEMPLATE.format(jsonstr=jsonstr)\n",
    "#display(HTML(html))\n",
    "\n",
    "file = open(\"babyweight-facest.html\",\"w\") \n",
    "file.write(html) \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<iframe width=\"100%\" height=\"600\" src=\"babyweight-facest.html\"></iframe>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Average Weight as a Baseline Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "avg_weight = data.weight_pounds.mean()\n",
    "print(\"Average Weight: {}\".format(round(avg_weight,3)))\n",
    "rmse = np.sqrt(data.weight_pounds.map(lambda value: (value-avg_weight)**2).mean())\n",
    "print(\"RMSE: {}\".format(round(rmse,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression with BigQuery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- BQML: Create BigQuery dataset for ML models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "\n",
    "BQML_DATASET = 'bqml_playground'\n",
    "BQML_ESTOMATPR_NAME = 'babyweight_estimator'\n",
    "BQML_DATASET_LOCATION = 'US'\n",
    "\n",
    "bq_client = bigquery.Client(PROJECT)\n",
    "dataset_ref = bq_client.dataset(BQML_DATASET)\n",
    "\n",
    "dataset = bigquery.Dataset(dataset_ref)\n",
    "\n",
    "\n",
    "if BQML_DATASET in list(map(lambda dataset: dataset.dataset_id,bq_client.list_datasets())):\n",
    "    print('Deleting BQ Dataset {}...'.format(BQML_DATASET))\n",
    "    bq_client.delete_dataset(dataset=dataset, delete_contents=True)\n",
    "    \n",
    "print('Creating BQ Table {}...'.format(BQML_DATASET))\n",
    "dataset.location = BQML_DATASET_LOCATION\n",
    "bq_client.create_dataset(dataset=dataset)\n",
    "\n",
    "print('BQ Dataset {} is up and running'.format(BQML_DATASET))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- BQML: Create and train the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "bqml_train_query = (\n",
    "'''\n",
    "CREATE MODEL {}.{} \n",
    "  OPTIONS( model_type='linear_reg',\n",
    "    learn_rate=0.1, \n",
    "    #l1_reg=0.001,\n",
    "    max_iteration=1000,\n",
    "    labels=['weight_pounds']\n",
    "  ) AS\n",
    "SELECT\n",
    "  ROUND(weight_pounds,1) AS weight_pounds,\n",
    "  COALESCE(CAST(is_male AS STRING),'NA') is_male,\n",
    "  mother_age,\n",
    "  COALESCE(CAST(mother_race AS STRING),'NA') mother_race,\n",
    "  plurality,\n",
    "  gestation_weeks,\n",
    "  mother_married,\n",
    "  COALESCE(CAST(cigarette_use AS STRING),'NA') cigarette_use,\n",
    "  COALESCE(CAST(alcohol_use AS STRING),'NA') alcohol_use\n",
    "FROM\n",
    "  publicdata.samples.natality\n",
    "WHERE\n",
    "  year = 2000\n",
    "  AND weight_pounds > 0\n",
    "  AND mother_age > 0\n",
    "  AND plurality > 0\n",
    "  AND gestation_weeks > 0\n",
    "  AND month > 0\n",
    "LIMIT\n",
    "  10000;\n",
    "'''.format(BQML_DATASET, BQML_ESTOMATPR_NAME)\n",
    ")\n",
    "\n",
    "#print bqml_train_query\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"Training started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\") \n",
    "\n",
    "query_job = bq_client.query(\n",
    "    query=bqml_train_query,\n",
    "    location=BQML_DATASET_LOCATION\n",
    ") \n",
    "print \"Status: {}\".format(query_job.state)\n",
    "\n",
    "try:\n",
    "    results = query_job.result()\n",
    "    print results\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print \"Status: {}\".format(query_job.state)\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Training finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Training elapsed time: {} seconds\".format(time_elapsed.total_seconds()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- BQML: Get Predictions using the Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "bqml_predict_query = (\n",
    "'''\n",
    "SELECT \n",
    "    ROUND(predicted_label,1) estimated_weight,\n",
    "    weight_pounds\n",
    "FROM ml.PREDICT(\n",
    "  MODEL {}.{}, \n",
    "  (\n",
    "      SELECT\n",
    "          ROUND(weight_pounds,1) AS weight_pounds,\n",
    "          COALESCE(CAST(is_male AS STRING),'NA') is_male,\n",
    "          mother_age,\n",
    "          COALESCE(CAST(mother_race AS STRING),'NA') mother_race,\n",
    "          plurality,\n",
    "          gestation_weeks,\n",
    "          mother_married,\n",
    "          COALESCE(CAST(cigarette_use AS STRING),'NA') cigarette_use,\n",
    "          COALESCE(CAST(alcohol_use AS STRING),'NA') alcohol_use\n",
    "      FROM\n",
    "        publicdata.samples.natality\n",
    "      WHERE\n",
    "        year = 2000\n",
    "        AND weight_pounds > 0\n",
    "        AND mother_age > 0\n",
    "        AND plurality > 0\n",
    "        AND gestation_weeks > 0\n",
    "          AND month > 0\n",
    "     LIMIT 10\n",
    "   )\n",
    ");\n",
    "\n",
    "'''.format(BQML_DATASET, BQML_ESTOMATPR_NAME)\n",
    ")\n",
    "\n",
    "#print bqml_predict_query\n",
    "\n",
    "query_job = bq_client.query(\n",
    "    query=bqml_predict_query,\n",
    "    location=BQML_DATASET_LOCATION\n",
    ") \n",
    "print \"Status: {}\".format(query_job.state)\n",
    "\n",
    "results = query_job.result()\n",
    "for row in results:\n",
    "    print(\"Predicted:{},  Actual: {}\".format(row.estimated_weight, row.weight_pounds))\n",
    "\n",
    "print \"Status: {}\".format(query_job.state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create ML dataset using Dataflow\n",
    "\n",
    "Let's use Cloud Dataflow to preprocess the data. The pipeline should do the following steps:\n",
    "1. Read the data from BigQuery \n",
    "2. Clean, process, and transform the data to CSV\n",
    "2. Write the results to files in GCS \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Define the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import datetime\n",
    "\n",
    "dataset_size = 100000\n",
    "train_size = dataset_size * 0.7\n",
    "eval_size = dataset_size * 0.3\n",
    "\n",
    "query = \"\"\"\n",
    "    SELECT\n",
    "      ROUND(weight_pounds,1) AS weight_pounds ,\n",
    "      is_male,\n",
    "      mother_age,\n",
    "      mother_race,\n",
    "      plurality,\n",
    "      gestation_weeks,\n",
    "      mother_married,\n",
    "      cigarette_use,\n",
    "      alcohol_use,\n",
    "      FARM_FINGERPRINT( \n",
    "        CONCAT(\n",
    "          COALESCE(CAST(weight_pounds AS STRING), 'NA'),\n",
    "          COALESCE(CAST(is_male AS STRING),'NA'),\n",
    "          COALESCE(CAST(mother_age AS STRING),'NA'),\n",
    "          COALESCE(CAST(mother_race AS STRING),'NA'),\n",
    "          COALESCE(CAST(plurality AS STRING), 'NA'),\n",
    "          COALESCE(CAST(gestation_weeks AS STRING),'NA'),\n",
    "          COALESCE(CAST(mother_married AS STRING), 'NA'),\n",
    "          COALESCE(CAST(cigarette_use AS STRING),'NA'),\n",
    "          COALESCE(CAST(alcohol_use AS STRING),'NA')\n",
    "          )\n",
    "        ) AS key\n",
    "        FROM\n",
    "          publicdata.samples.natality\n",
    "        WHERE year > 2000\n",
    "        AND weight_pounds > 0\n",
    "        AND mother_age > 0\n",
    "        AND plurality > 0\n",
    "        AND gestation_weeks > 0\n",
    "        AND month > 0\n",
    "    \"\"\"\n",
    "\n",
    "out_dir = gcs_data_dir + \"big_data\"\n",
    "\n",
    "def to_csv(bq_row):\n",
    "    # pull columns from BQ and create a line\n",
    "    import hashlib\n",
    "    import copy\n",
    "    CSV_COLUMNS = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use,key'.split(',')\n",
    "    # modify opaque numeric race code into human-readable data\n",
    "    races = dict(zip([1,2,3,4,5,6,7,18,28,39,48],\n",
    "                     ['White', 'Black', 'American Indian', 'Chinese', \n",
    "                      'Japanese', 'Hawaiian', 'Filipino',\n",
    "                      'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']))\n",
    "    result = copy.deepcopy(bq_row)\n",
    "    if 'mother_race' in bq_row and bq_row['mother_race'] in races:\n",
    "        result['mother_race'] = races[bq_row['mother_race']]\n",
    "    else:\n",
    "        result['mother_race'] = 'Unknown'\n",
    "    \n",
    "    csv_data = ','.join([str(result[k]) if k in result else 'None' for k in CSV_COLUMNS])\n",
    "    return csv_data\n",
    "  \n",
    "def run_pipeline(runner, opts):\n",
    "  \n",
    "    pipeline = beam.Pipeline(RUNNER, options=opts)\n",
    "    \n",
    "    for step in ['train', 'eval']:\n",
    "        \n",
    "        if step == 'train':\n",
    "            source_query = 'SELECT * FROM ({}) WHERE MOD(key,4) < 3 LIMIT {}'.format(query,int(train_size))\n",
    "        else:\n",
    "            source_query = 'SELECT * FROM ({}) WHERE MOD(key,4) = 3 LIMIT {}'.format(query,int(eval_size))\n",
    "            \n",
    "        sink_location = os.path.join(out_dir, '{}-data'.format(step))\n",
    "\n",
    "        (\n",
    "            pipeline \n",
    "           | '{} - Read from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "           | '{} - Process to CSV'.format(step) >> beam.Map(to_csv)\n",
    "           | '{} - Write to GCS '.format(step) >> beam.io.Write(beam.io.WriteToText(sink_location,\n",
    "                                                                file_name_suffix='.csv',\n",
    "                                                                num_shards=5\n",
    "                                                                                   ))\n",
    "        )\n",
    "        \n",
    "    job = pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Run the Pipeline on Dataflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_name = 'preprocess-babyweight-data' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')\n",
    "\n",
    "options = {\n",
    "    'region': 'europe-west1',\n",
    "    'staging_location': os.path.join(out_dir, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(out_dir, 'tmp'),\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT\n",
    "}\n",
    "\n",
    "opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "RUNNER = 'DataflowRunner'\n",
    "\n",
    "print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "\n",
    "run_pipeline(RUNNER, opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil ls gs://ksalama-gcs-cloudml/data/babyweight/big_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create TensorFlow Models using Estimator APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment A: Train Linear Regression Model\n",
    "\n",
    "1. Define dataset metadata + data input function\n",
    "\n",
    "2. Create feature columns based on metadata\n",
    "\n",
    "3. Instantiate the model with feature columns \n",
    "\n",
    "4. Train, evaluate, and predict using the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Define Metadata &  Input Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADER = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use,key'.split(',')\n",
    "TARGET_FEATURE_NAME = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "DEFAULTS = [[0.0], ['null'], [0.0], ['null'], [0.0], [0.0], ['null'], ['null'], ['null'], ['nokey']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv_row(csv_row):\n",
    "    \n",
    "    columns = tf.decode_csv(tf.expand_dims(csv_row, -1), record_defaults=DEFAULTS)\n",
    "    features = dict(zip(HEADER, columns))\n",
    "    features.pop(KEY_COLUMN)\n",
    "    target = features.pop(TARGET_FEATURE_NAME)\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_input_fn(file_name, mode=tf.estimator.ModeKeys.EVAL, \n",
    "                 skip_header_lines=0, \n",
    "                 num_epochs=1, \n",
    "                 batch_size=500):\n",
    "    \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    file_names = tf.matching_files(file_name)\n",
    "\n",
    "    dataset = data.TextLineDataset(filenames=file_names)\n",
    "    dataset = dataset.skip(skip_header_lines)\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda csv_row: parse_csv_row(csv_row))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, target = iterator.get_next()\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Create Feature Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_columns():\n",
    "\n",
    "    is_male=tf.feature_column.categorical_column_with_vocabulary_list('is_male', ['True', 'False'])\n",
    "    mother_age=tf.feature_column.numeric_column('mother_age')\n",
    "    mother_race=tf.feature_column.categorical_column_with_vocabulary_list('mother_race', ['White', 'Black', 'American Indian', 'Chinese', \n",
    "               'Japanese', 'Hawaiian', 'Filipino', 'Unknown', 'Asian Indian', 'Korean', 'Samaon', 'Vietnamese'])\n",
    "    plurality=tf.feature_column.numeric_column('plurality')\n",
    "    gestation_weeks=tf.feature_column.numeric_column('gestation_weeks')\n",
    "    mother_married=tf.feature_column.categorical_column_with_vocabulary_list('mother_married', ['True', 'False'])\n",
    "    cigarette_use=tf.feature_column.categorical_column_with_vocabulary_list('cigarette_use', ['True', 'False', 'None'])\n",
    "    alcohol_use=tf.feature_column.categorical_column_with_vocabulary_list('alcohol_use', ['True', 'False', 'None'])\n",
    "    \n",
    "    feature_columns = [is_male, mother_age, mother_race, plurality, gestation_weeks, mother_married, cigarette_use, alcohol_use]\n",
    "    \n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Instantiate a Linear Regression Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = os.path.join(local_models_dir,\"lr_estimator\")\n",
    "\n",
    "feature_columns = create_feature_columns()\n",
    "\n",
    "lr_estimator = tf.estimator.LinearRegressor(feature_columns=feature_columns,\n",
    "                                            model_dir=model_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Train, Evaluate, and Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls data/babyweight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "train_data_files = \"data/babyweight/train.csv\"\n",
    "\n",
    "train_input_fn = lambda: csv_input_fn(train_data_files, \n",
    "                                              mode=tf.estimator.ModeKeys.TRAIN, \n",
    "                                              num_epochs=10,\n",
    "                                              batch_size = 200\n",
    "                                         )\n",
    "\n",
    "# remove the following line of code to resume training\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "lr_estimator.train(train_input_fn, max_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls models/babyweight/lr_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Evaluate the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data_files = \"data/babyweight/eval.csv\"\n",
    "\n",
    "eval_input_fn =lambda: csv_input_fn(eval_data_files)\n",
    "\n",
    "lr_estimator.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) Predict using the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "predictions = lr_estimator.predict(eval_input_fn)\n",
    "values = list(map(lambda item: item[\"predictions\"][0],list(itertools.islice(predictions, 5))))\n",
    "print(\"\")\n",
    "print(\"Predicted Values: {}\".format(values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment B:  Wide and Deep DNN Model \n",
    "In this experiement, we are going to:\n",
    "1. Preprocess the data using tf.transform\n",
    "2. Build a DNNLinearCombinedRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.1 Prepare the data using tf.transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import tensorflow_transform.coders as tft_coders\n",
    "\n",
    "from tensorflow.contrib.learn.python.learn.utils import input_fn_utils\n",
    "\n",
    "from tensorflow_transform.beam import impl\n",
    "from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "from tensorflow_transform.tf_metadata import metadata_io\n",
    "from tensorflow_transform.tf_metadata import dataset_schema\n",
    "from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "from tensorflow_transform.saved import saved_transform_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Raw Data Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_FEATURE_NAMES = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use,key'.split(',')\n",
    "CATEGORICAL_FEATURE_NAMES = 'is_male,mother_race,mother_married,cigarette_use,alcohol_use'.split(',')\n",
    "NUMERIC_FEATURE_NAMES = 'mother_age,plurality,gestation_weeks'.split(',')\n",
    "TARGET_FEATURE_NAME = 'weight_pounds'\n",
    "KEY_COLUMN = 'key'\n",
    "\n",
    "def create_raw_metadata():  \n",
    "    \n",
    "    raw_data_schema = {}\n",
    "    \n",
    "    raw_data_schema[KEY_COLUMN]= dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    raw_data_schema[TARGET_FEATURE_NAME]= dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "    \n",
    "    raw_data_schema.update(\n",
    "        { column_name : dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for column_name in CATEGORICAL_FEATURE_NAMES\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    raw_data_schema.update(\n",
    "        { column_name : dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for column_name in NUMERIC_FEATURE_NAMES\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    raw_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
    "    \n",
    "    return raw_metadata\n",
    "\n",
    "print(create_raw_metadata().schema.as_feature_spec())\n",
    "#print(create_raw_metadata().schema._column_schemas.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Source Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_size = 1000#00\n",
    "train_size = dataset_size * 0.7\n",
    "eval_size = dataset_size * 0.3\n",
    "\n",
    "def get_source_query(step):\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT\n",
    "      ROUND(weight_pounds,1) AS weight_pounds,\n",
    "      is_male,\n",
    "      mother_age,\n",
    "      mother_race,\n",
    "      plurality,\n",
    "      gestation_weeks,\n",
    "      mother_married,\n",
    "      cigarette_use,\n",
    "      alcohol_use,\n",
    "      FARM_FINGERPRINT( \n",
    "        CONCAT(\n",
    "          COALESCE(CAST(weight_pounds AS STRING), 'NA'),\n",
    "          COALESCE(CAST(is_male AS STRING),'NA'),\n",
    "          COALESCE(CAST(mother_age AS STRING),'NA'),\n",
    "          COALESCE(CAST(mother_race AS STRING),'NA'),\n",
    "          COALESCE(CAST(plurality AS STRING), 'NA'),\n",
    "          COALESCE(CAST(gestation_weeks AS STRING),'NA'),\n",
    "          COALESCE(CAST(mother_married AS STRING), 'NA'),\n",
    "          COALESCE(CAST(cigarette_use AS STRING),'NA'),\n",
    "          COALESCE(CAST(alcohol_use AS STRING),'NA')\n",
    "          )\n",
    "        ) AS key\n",
    "        FROM\n",
    "          publicdata.samples.natality\n",
    "        WHERE year > 2000\n",
    "        AND weight_pounds > 0\n",
    "        AND mother_age > 0\n",
    "        AND plurality > 0\n",
    "        AND gestation_weeks > 0\n",
    "        AND month > 0\n",
    "    \"\"\"\n",
    "    \n",
    "    if step == 'train':\n",
    "        source_query = 'SELECT * FROM ({}) WHERE MOD(key,4) < 3 LIMIT {}'.format(query,int(train_size))\n",
    "    else:\n",
    "        source_query = 'SELECT * FROM ({}) WHERE MOD(key,4) = 3 LIMIT {}'.format(query,int(eval_size))\n",
    "    \n",
    "    return source_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup(bq_row):\n",
    "    \n",
    "    RAW_FEATURE_NAMES = 'weight_pounds,is_male,mother_age,mother_race,plurality,gestation_weeks,mother_married,cigarette_use,alcohol_use,key'.split(',')\n",
    "    \n",
    "    # modify opaque numeric race code into human-readable data\n",
    "    races = dict(zip([1,2,3,4,5,6,7,18,28,39,48],\n",
    "                     ['White', 'Black', 'American Indian', 'Chinese', \n",
    "                      'Japanese', 'Hawaiian', 'Filipino',\n",
    "                      'Asian Indian', 'Korean', 'Samaon', 'Vietnamese']))\n",
    "    result = {} \n",
    "    \n",
    "    for feature_name in RAW_FEATURE_NAMES:\n",
    "        result[feature_name] = str(bq_row[feature_name])\n",
    "\n",
    "    if 'mother_race' in bq_row and bq_row['mother_race'] in races:\n",
    "        result['mother_race'] = races[bq_row['mother_race']]\n",
    "    else:\n",
    "        result['mother_race'] = 'Unknown'\n",
    "\n",
    "    return result\n",
    "\n",
    "def preprocess_tft(input_features):\n",
    "    \n",
    "    output_features = {}\n",
    "    \n",
    "    output_features['key'] = input_features['key']\n",
    "    output_features['weight_pounds'] = input_features['weight_pounds']\n",
    "\n",
    "    # normalisation\n",
    "    output_features['mother_age_normalized'] = tft.scale_to_z_score(input_features['mother_age'])\n",
    "    \n",
    "    # bucktisation based on quantiles\n",
    "    age_buckets = tft.quantiles(input_features['mother_age'], num_buckets=5, epsilon=0.01)\n",
    "    output_features['mother_age_bucketized'] = tft.apply_buckets(input_features['mother_age'], age_buckets)\n",
    "    \n",
    "    # scaling between 0 and 1\n",
    "    output_features['gestation_weeks_scaled'] =  tft.scale_to_0_1(input_features['gestation_weeks'])\n",
    "    \n",
    "    # you can compute new features based on custom formulas\n",
    "    output_features['mother_age_log'] = tf.log(input_features['mother_age'])\n",
    "    \n",
    "    # or create flags/indicators\n",
    "    output_features['is_multiple'] = tf.cast(input_features['plurality'] > [1], dtype=tf.int64)\n",
    "    \n",
    "    # extract vocab from categorical columns\n",
    "    for feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "        tft.uniques(input_features[feature_name], vocab_filename=feature_name)\n",
    "        output_features[feature_name] = input_features[feature_name]\n",
    "        \n",
    "    return output_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Transformation Beam Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_DIR = 'tmp'\n",
    "TRANSFORM_ARTEFACTS_DIR = 'transform'\n",
    "TRANSFORMED_DATA_DIR = 'transformed'\n",
    "\n",
    "def run_transformation_pipeline(runner='DirectRunner', options=None):\n",
    "    \n",
    "    pipeline_options = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    \n",
    "    sink_transformed_data_location = os.path.join(gcs_data_dir if runner=='DataflowRunner' else local_data_dir, \n",
    "                                                  TRANSFORMED_DATA_DIR)\n",
    "    \n",
    "    sink_transform_dir = os.path.join(gcs_model_dir if runner=='DataflowRunner' else local_models_dir,\n",
    "                                      TRANSFORM_ARTEFACTS_DIR)\n",
    "    \n",
    "    temporary_dir = os.path.join(gcs_data_dir if runner=='DataflowRunner' else local_data_dir,\n",
    "                                      TEMP_DIR)\n",
    "    \n",
    "    print(\"Sink data files prefix: {}\".format(sink_transformed_data_location))\n",
    "    print(\"Sink transformation directory: {}\".format(sink_transform_dir))\n",
    "    print(\"Temporary directory: {}\".format(temporary_dir))\n",
    "    \n",
    "    opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "    \n",
    "    with beam.Pipeline(runner, options=opts) as pipeline:\n",
    "        with impl.Context(temporary_dir):\n",
    "            \n",
    "            raw_metadata = create_raw_metadata()\n",
    "\n",
    "            ###### analyze & transform train #########################################################\n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"\")\n",
    "                print(\"Transform training data....\")\n",
    "                print(\"\")\n",
    "            \n",
    "            step = 'train'\n",
    "            source_query = get_source_query(step)\n",
    "            train_transformed_data_location = os.path.join(sink_transformed_data_location,'{}-data'.format(step))\n",
    "            \n",
    "            # Read raw train data from BQ and cleanup\n",
    "            raw_train_data = (\n",
    "              pipeline\n",
    "              | '{} - Read Data from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "              | '{} - Clean up Data'.format(step) >> beam.Map(cleanup)\n",
    "            )\n",
    "            \n",
    "            # create a train dataset from the data and schema\n",
    "            raw_train_dataset = (raw_train_data, raw_metadata)\n",
    "            \n",
    "            # analyze and transform raw_train_dataset to produced transformed_train_dataset and transform_fn\n",
    "            transformed_train_dataset, transform_fn = (\n",
    "                raw_train_dataset \n",
    "                | '{} - Analyze & Transform'.format(step) >> impl.AnalyzeAndTransformDataset(preprocess_tft)\n",
    "            )\n",
    "            \n",
    "            # get data and schema separately from the transformed_train_dataset\n",
    "            transformed_train_data, transformed_metadata = transformed_train_dataset\n",
    "\n",
    "            # write transformed train data to sink\n",
    "            _ = (\n",
    "                transformed_train_data \n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=train_transformed_data_location,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    coder=tft_coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "            \n",
    "            ###### transform eval ##################################################################\n",
    "            \n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"\")\n",
    "                print(\"Transform eval data....\")\n",
    "                print(\"\")\n",
    "            \n",
    "            step = 'eval'\n",
    "            source_query = get_source_query(step)\n",
    "            eval_transformed_data_location = os.path.join(sink_transformed_data_location,'{}-data-'.format(step))\n",
    "            \n",
    "            # Read raw eval data from BQ and cleanup\n",
    "            raw_eval_data = (\n",
    "              pipeline\n",
    "              | '{} - Read Data from BigQuery'.format(step) >> beam.io.Read(beam.io.BigQuerySource(query=source_query, use_standard_sql=True))\n",
    "              | '{} - Clean up Data'.format(step) >> beam.Map(cleanup)\n",
    "            )\n",
    "            \n",
    "            # create a eval dataset from the data and schema\n",
    "            raw_eval_dataset = (raw_eval_data, raw_metadata)\n",
    "            \n",
    "            # transform eval data based on produced transform_fn (from analyzing train_data)\n",
    "            transformed_eval_dataset = (\n",
    "                (raw_eval_dataset, transform_fn) \n",
    "                | '{} - Transform'.format(step) >> impl.TransformDataset()\n",
    "            )\n",
    "            \n",
    "            # get data from the transformed_eval_dataset\n",
    "            transformed_eval_data, _ = transformed_eval_dataset\n",
    "            \n",
    "            # write transformed eval data to sink\n",
    "            _ = (\n",
    "                transformed_eval_data \n",
    "                | '{} - Write Transformed Data'.format(step) >> beam.io.tfrecordio.WriteToTFRecord(\n",
    "                    file_path_prefix=eval_transformed_data_location,\n",
    "                    file_name_suffix=\".tfrecords\",\n",
    "                    coder=tft_coders.example_proto_coder.ExampleProtoCoder(transformed_metadata.schema))\n",
    "            )\n",
    "        \n",
    "            ###### write transformation metadata #######################################################\n",
    "            if(runner=='DirectRunner'):\n",
    "                print(\"\")\n",
    "                print(\"Saving transformation artefacts ....\")\n",
    "                print(\"\")\n",
    "            \n",
    "            # write transform_fn as tf.graph\n",
    "            _ = (\n",
    "                transform_fn \n",
    "                | 'Write Transform Artefacts' >> transform_fn_io.WriteTransformFn(sink_transform_dir)\n",
    "            )\n",
    "\n",
    "    if runner=='DataflowRunner':\n",
    "        pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Run Transformation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "tensorflow-transform==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "gsutil -m rm -r gs://ksalama-gcs-cloudml/data/babyweight/transformed\n",
    "gsutil -m rm -r gs://ksalama-gcs-cloudml/models/babyweight/transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.WARN)\n",
    "\n",
    "runner = 'DirectRunner' # DirectRunner | DataflowRunner\n",
    "\n",
    "job_name = 'preprocess-babweight-data-tft-{}'.format(datetime.utcnow().strftime('%y%m%d-%H%M%S'))\n",
    "print 'Launching {} job {} ... hang on'.format(runner, job_name)\n",
    "print(\"\")\n",
    "\n",
    "options = {\n",
    "    'region': 'europe-west1',\n",
    "    'staging_location': os.path.join(gcs_data_dir, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(gcs_data_dir, 'tmp'),\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT,\n",
    "    'worker_machine_type': 'n1-standard-1',\n",
    "    #'num_workers': 1,\n",
    "    'max_num_workers': 20,\n",
    "    'requirements_file': 'requirements.txt'\n",
    "}\n",
    "\n",
    "if runner == 'DirectRunner':\n",
    "    \n",
    "    shutil.rmtree(os.path.join(local_models_dir,TRANSFORM_ARTEFACTS_DIR), ignore_errors=True)\n",
    "    shutil.rmtree(os.path.join(local_data_dir,TRANSFORMED_DATA_DIR), ignore_errors=True)\n",
    "    shutil.rmtree(os.path.join(local_data_dir,TEMP_DIR), ignore_errors=True)\n",
    "\n",
    "run_transformation_pipeline(runner, options)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"***Transformed Data:\"\n",
    "ls data/babyweight/transformed\n",
    "echo \"\"\n",
    "\n",
    "echo \"***Transform Artefacts:\"\n",
    "ls models/babyweight/transform\n",
    "echo \"\"\n",
    "\n",
    "echo \"***Transform function:\"\n",
    "ls models/babyweight/transform/transform_fn\n",
    "echo \"\"\n",
    "\n",
    "echo \"***Transform assets:\"\n",
    "ls models/babyweight/transform/transform_fn/assets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.2: Train a DNN Liner Combined Regression Model + Feature Engineering\n",
    "\n",
    "1. Define dataset metadata + input function (to read and parse the data files, + **process features**) \n",
    "\n",
    "2. Create feature columns based on metadata + **Extended Feature Columns**\n",
    "\n",
    "3. Initialise the Estimator + **Wide & Deep Columns for the combined DNN model**\n",
    "\n",
    "4. Setup an experiment with TrainSpec, EvalSepc, Serving_fn, run_config, and params\n",
    "\n",
    "5. Run **train_and_evaluate** experiment \n",
    "\n",
    "6. Use the SavedModel for predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import data\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1- Define input function with process features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_metadata = metadata_io.read_metadata(\n",
    "    os.path.join(local_models_dir,TRANSFORM_ARTEFACTS_DIR,\"transformed_metadata\"))\n",
    "\n",
    "transformed_feature_spec = transformed_metadata.schema.as_feature_spec()\n",
    "\n",
    "print(transformed_feature_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tf_example(example_proto):\n",
    "    \n",
    "    parsed_features = tf.parse_example(serialized=example_proto, features=transformed_feature_spec)\n",
    "    parsed_features.pop(KEY_COLUMN)\n",
    "    target = parsed_features.pop(TARGET_FEATURE_NAME)\n",
    "    \n",
    "    return parsed_features, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be applied in traing and serving\n",
    "# ideally, you put this logic in preprocess_tft, to avoid transforming the records during training several times\n",
    "\n",
    "def process_features(features):\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfrecords_input_fn(files_name_pattern, mode=tf.estimator.ModeKeys.EVAL,  \n",
    "                 num_epochs=1, \n",
    "                 batch_size=500):\n",
    "    \n",
    "    shuffle = True if mode == tf.estimator.ModeKeys.TRAIN else False\n",
    "    \n",
    "    file_names = data.Dataset.list_files(files_name_pattern)\n",
    "\n",
    "    dataset = data.TFRecordDataset(filenames=file_names)\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=2 * batch_size + 1)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(lambda tf_example: parse_tf_example(tf_example))\n",
    "    dataset = dataset.map(lambda features, target: (process_features(features), target))\n",
    "    dataset = dataset.repeat(num_epochs)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    features, target = iterator.get_next()\n",
    "    return features, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2- Create Feature Columns with Extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deep_and_wide_columns():\n",
    "\n",
    "    assets_dir = os.path.join(local_models_dir, TRANSFORM_ARTEFACTS_DIR, 'transform_fn/assets')\n",
    "    \n",
    "    categorical_feature_columns = {feature_name: \n",
    "      tf.feature_column.categorical_column_with_vocabulary_file(feature_name, vocabulary_file=os.path.join(assets_dir,feature_name ))\n",
    "      for feature_name in CATEGORICAL_FEATURE_NAMES}\n",
    "    \n",
    "    is_multiple = tf.feature_column.categorical_column_with_identity('is_multiple', num_buckets=2)\n",
    "    gestation_weeks_scaled =  tf.feature_column.numeric_column('gestation_weeks_scaled')\n",
    "    mother_age_log = tf.feature_column.numeric_column('mother_age_log')\n",
    "    mother_age_normalized = tf.feature_column.numeric_column('mother_age_normalized')\n",
    "    \n",
    "    # extended feature columns\n",
    "    cigarette_use_X_alcohol_use = tf.feature_column.crossed_column(\n",
    "      [categorical_feature_columns['cigarette_use'], categorical_feature_columns['alcohol_use']], 9)\n",
    "    \n",
    "    #mother_age_bucketized = tf.feature_column.bucketized_column(mother_age, boundaries=[18, 22, 28, 32, 36, 40, 42, 45, 50])\n",
    "    mother_age_bucketized = tf.feature_column.categorical_column_with_identity('mother_age_bucketized', num_buckets=5)\n",
    "    \n",
    "    mother_race_X_mother_age_bucketized = tf.feature_column.crossed_column( [mother_age_bucketized,categorical_feature_columns['mother_race']],  120)\n",
    "    \n",
    "    mother_race_X_mother_age_bucketized_embedded = tf.feature_column.embedding_column(mother_race_X_mother_age_bucketized, 5)\n",
    "    \n",
    "    # wide and deep columns\n",
    "    wide_columns = categorical_feature_columns.values() + [is_multiple, cigarette_use_X_alcohol_use, mother_age_bucketized, mother_race_X_mother_age_bucketized] \n",
    "    deep_columns = [mother_age_log, gestation_weeks_scaled, mother_race_X_mother_age_bucketized_embedded]\n",
    "    \n",
    "    return wide_columns, deep_columns\n",
    "\n",
    "# w,d = get_deep_and_wide_columns()\n",
    "# print w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3- Create a DNN Regression Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_fn(labels, predictions):\n",
    "\n",
    "    metrics = {}\n",
    "    \n",
    "    pred_values = predictions['predictions']\n",
    "    \n",
    "    metrics['rmse'] = tf.metrics.root_mean_squared_error(\n",
    "      labels=labels,\n",
    "      predictions=pred_values)\n",
    "    \n",
    "    metrics['mae'] = tf.metrics.mean_absolute_error(\n",
    "      labels=labels,\n",
    "      predictions=pred_values)\n",
    "    \n",
    "    \n",
    "    return metrics\n",
    "\n",
    "\n",
    "def create_DNNLinearCombinedRegressor(run_config, hparams):\n",
    "  \n",
    "    wide_columns, deep_columns = get_deep_and_wide_columns()\n",
    "\n",
    "    dnn_optimizer = tf.train.AdamOptimizer(learning_rate=hparams.learning_rate)\n",
    "    \n",
    "    estimator = tf.estimator.DNNLinearCombinedRegressor(\n",
    "                linear_feature_columns = wide_columns,\n",
    "                dnn_feature_columns = deep_columns,\n",
    "                dnn_optimizer=dnn_optimizer,\n",
    "                dnn_hidden_units=hparams.hidden_units,\n",
    "                config = run_config\n",
    "                )\n",
    "    \n",
    "    \n",
    "    estimator = tf.contrib.estimator.add_metrics(estimator, metric_fn)\n",
    "    \n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4- Setup Local Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) RunConfig and Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "hparams  = tf.contrib.training.HParams(\n",
    "    num_epochs=10,\n",
    "    batch_size=500,\n",
    "    hidden_units=[32, 16],\n",
    "    max_steps=100,\n",
    "    learning_rate=0.1,\n",
    "    evaluate_after_sec=10\n",
    ")\n",
    "\n",
    "# RunConfig\n",
    "model_dir = os.path.join(local_models_dir,\"dnn_estimator\")\n",
    "\n",
    "run_config = tf.estimator.RunConfig(\n",
    "    tf_random_seed=19830610,\n",
    "    model_dir=model_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Serving Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_serving_input_fn():\n",
    "    \n",
    "    def _serving_fn():\n",
    "        \n",
    "        # get the feature_spec of raw data\n",
    "        raw_metadata = create_raw_metadata()\n",
    "        raw_placeholder_spec = raw_metadata.schema.as_batched_placeholders()\n",
    "        raw_placeholder_spec.pop(TARGET_FEATURE_NAME)\n",
    "    \n",
    "        raw_input_fn = tf.estimator.export.build_raw_serving_input_receiver_fn(raw_placeholder_spec)\n",
    "        raw_features, recevier_tensors, _ = raw_input_fn()\n",
    "        \n",
    "        # apply tranform_fn on raw features\n",
    "        _, transformed_features = (\n",
    "            saved_transform_io.partially_apply_saved_transform(\n",
    "                os.path.join(local_models_dir,TRANSFORM_ARTEFACTS_DIR,transform_fn_io.TRANSFORM_FN_DIR),\n",
    "            raw_features)\n",
    "        )\n",
    "        \n",
    "        # apply the process_features function to transformed features\n",
    "        transformed_features = process_features(transformed_features)\n",
    "        \n",
    "        return tf.estimator.export.ServingInputReceiver(\n",
    "            transformed_features, raw_features)\n",
    "    \n",
    "    return _serving_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c) TrainSpec and EvalSpec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_files = os.path.join(local_data_dir,TRANSFORMED_DATA_DIR)+\"/train-*.tfrecords\"\n",
    "eval_data_files = os.path.join(local_data_dir,TRANSFORMED_DATA_DIR)+\"/eval-*.tfrecords\"\n",
    "\n",
    "# TrainSpec\n",
    "train_spec = tf.estimator.TrainSpec(\n",
    "  input_fn = lambda: tfrecords_input_fn(\n",
    "    train_data_files,\n",
    "    mode=tf.estimator.ModeKeys.TRAIN,\n",
    "    num_epochs= hparams.num_epochs,\n",
    "    batch_size = hparams.batch_size\n",
    "  ),\n",
    "  max_steps=hparams.max_steps,\n",
    ")\n",
    "\n",
    "# EvalSpec\n",
    "eval_spec = tf.estimator.EvalSpec(\n",
    "  input_fn =lambda: tfrecords_input_fn(eval_data_files),\n",
    "  exporters=[tf.estimator.LatestExporter(\n",
    "      name=\"estimate\",  # the name of the folder in which the model will be exported to under export\n",
    "      serving_input_receiver_fn=generate_serving_input_fn(),\n",
    "      exports_to_keep=1,\n",
    "      as_text=True)],\n",
    "  steps = None,\n",
    "  throttle_secs = hparams.evaluate_after_sec # evalute after each 10 training seconds!\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> TensorBoard - Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.datalab.ml import TensorBoard\n",
    "TensorBoard().start(model_dir)\n",
    "TensorBoard().list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5- Run train_and_evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "# remove the following line of code to resume training\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "dnn_estimator = create_DNNLinearCombinedRegressor(run_config, hparams)\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "time_start = datetime.utcnow() \n",
    "print(\"\")\n",
    "print(\"Experiment started at {}\".format(time_start.strftime(\"%H:%M:%S\")))\n",
    "print(\".......................................\") \n",
    "\n",
    "# run train and evaluate experiment\n",
    "tf.estimator.train_and_evaluate(\n",
    "  dnn_estimator,\n",
    "  train_spec,\n",
    "  eval_spec\n",
    ")\n",
    "\n",
    "\n",
    "time_end = datetime.utcnow() \n",
    "print(\".......................................\")\n",
    "print(\"Experiment finished at {}\".format(time_end.strftime(\"%H:%M:%S\")))\n",
    "print(\"\")\n",
    "time_elapsed = time_end - time_start\n",
    "print(\"Experiment elapsed time: {} seconds\".format(time_elapsed.total_seconds()))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "ls models/babyweight/dnn_estimator/export/estimate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> TensorBoard - Stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to stop TensorBoard\n",
    "TensorBoard().stop(23002)\n",
    "print('stopped TensorBoard')\n",
    "TensorBoard().list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6- Use SavedModel for Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model_base_dir=os.path.join(model_dir,'export/estimate')\n",
    "SAVED_MODEL_DIR=os.path.join(saved_model_base_dir, os.listdir(saved_model_base_dir)[0])\n",
    "\n",
    "def estimate_local(instance):\n",
    " \n",
    "    predictor_fn = tf.contrib.predictor.from_saved_model(\n",
    "        export_dir=SAVED_MODEL_DIR,\n",
    "        signature_def_key=\"predict\"\n",
    "    )\n",
    "    \n",
    "    instance = dict((k, [v]) for k, v in instance.items())\n",
    "    value = predictor_fn(instance)['predictions'][0][0]\n",
    "    return value\n",
    "\n",
    "instance = {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "}\n",
    "\n",
    "prediction = estimate_local(instance)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate the model using TFMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_model_analysis as tfma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Evaluate input function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_eval_receiver_fn(transform_artefacts_dir):\n",
    "    \n",
    "    transformed_metadata = metadata_io.read_metadata(transform_artefacts_dir+\"/transformed_metadata\")\n",
    "    transformed_feature_spec = transformed_metadata.schema.as_feature_spec()\n",
    "    \n",
    "    def _eval_receiver_fn():\n",
    "        \n",
    "        serialized_tf_example = tf.placeholder(\n",
    "            dtype=tf.string, shape=[None], name='input_example_placeholder')\n",
    "\n",
    "        receiver_tensors = {'examples': serialized_tf_example}\n",
    "        transformed_features = tf.parse_example(serialized_tf_example, transformed_feature_spec)\n",
    "\n",
    "        return tfma.export.EvalInputReceiver(\n",
    "            features=transformed_features,\n",
    "            receiver_tensors=receiver_tensors,\n",
    "            labels=transformed_features[TARGET_FEATURE_NAME])\n",
    "\n",
    "    return _eval_receiver_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Export Evaluation Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model_dir = model_dir +\"/export/evaluate\"\n",
    "\n",
    "transform_artefacts_dir = os.path.join(local_models_dir,TRANSFORM_ARTEFACTS_DIR)\n",
    "\n",
    "tfma.export.export_eval_savedmodel(\n",
    "        estimator=dnn_estimator,\n",
    "        export_dir_base=eval_model_dir,\n",
    "        eval_input_receiver_fn=generate_eval_receiver_fn(transform_artefacts_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Produce Evaluation Results using the Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_spec = [tfma.SingleSliceSpec()]\n",
    "for feature_name, feature_spec in transformed_feature_spec.items():\n",
    "    if feature_name not in [KEY_COLUMN] + [TARGET_FEATURE_NAME] and feature_spec.dtype == tf.string:\n",
    "        slice_spec += [tfma.SingleSliceSpec(columns=[feature_name])]\n",
    "\n",
    "# print slice_spec\n",
    "# print \"\"\n",
    "\n",
    "saved_model_base_dir=os.path.join(model_dir,'export/evaluate')\n",
    "model_location=os.path.join(saved_model_base_dir, os.listdir(saved_model_base_dir)[0])\n",
    "data_location = os.path.join(local_data_dir, TRANSFORMED_DATA_DIR)+\"/eval-*.tfrecords\"\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "eval_results = tfma.run_model_analysis(\n",
    "    model_location=model_location , \n",
    "    data_location=data_location, \n",
    "    file_format='tfrecords', \n",
    "    slice_spec=slice_spec, \n",
    "#     output_path=None\n",
    ")\n",
    "\n",
    "print \"Evaluation results are ready!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Visualise the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print eval_results.slicing_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfma.view.render_slicing_metrics(\n",
    "        eval_results, \n",
    "    slicing_column='mother_age_bucketized'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train the Model on Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "TIER=BASIC # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=packages/babyweight-tf1.4/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/train-data.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/eval-data.csv\n",
    "MODEL_DIR=gs://${BUCKET}/models/babyweight/${MODEL_NAME}\n",
    "\n",
    "#remove model directory, if you don't want to resume training, or if you have changed the model structure\n",
    "#gsutil -m rm -r ${MODEL_DIR}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=train_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.4 \\\n",
    "        --region=${REGION} \\\n",
    "        --scale-tier=${TIER} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=100 \\\n",
    "        --train-batch-size=500 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=500 \\\n",
    "        --learning-rate=0.01 \\\n",
    "        --hidden-units=\"64,0,0\" \\\n",
    "        --layer-sizes-scale-factor=0.5 \\\n",
    "        --num-layers=3 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model on Cloud ML Engine + GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "TIER=BASIC_GPU # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=packages/babyweight-tf1.4/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/train-*.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/eval-*.csv\n",
    "MODEL_DIR=gs://${BUCKET}/models/babyweight/${MODEL_NAME}_${TIER}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=train_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.4 \\\n",
    "        --region=${REGION} \\\n",
    "        --scale-tier=${TIER} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=10 \\\n",
    "        --train-batch-size=1000 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=1000 \\\n",
    "        --learning-rate=0.01 \\\n",
    "        --hidden-units=\"64,0,0\" \\\n",
    "        --layer-sizes-scale-factor=0.5 \\\n",
    "        --num-layers=3 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model on Cloud ML Engine + Custom GPUs Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "TIER=CUSTOM # BASIC | BASIC_GPU | STANDARD_1 | PREMIUM_1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=packages/babyweight-tf1.4/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/big_data/train-*.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/big_data/eval-*.csv\n",
    "MODEL_DIR=gs://${BUCKET}/models/babyweight/${MODEL_NAME}_${TIER}\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=train_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.4 \\\n",
    "        --region=${REGION} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        --config=ml-packages/babyweight-tf1.4/custom.yaml \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=100 \\\n",
    "        --train-batch-size=1000 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=1000 \\\n",
    "        --learning-rate=0.001 \\\n",
    "        --hidden-units=\"64,0,0\" \\\n",
    "        --layer-sizes-scale-factor=0.5 \\\n",
    "        --num-layers=3 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters Tuning on Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "echo \"Submitting a Cloud ML Engine job...\"\n",
    "\n",
    "REGION=europe-west1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "\n",
    "PACKAGE_PATH=packages/babyweight-tf1.4/trainer\n",
    "TRAIN_FILES=gs://${BUCKET}/data/babyweight/big_data/train-*.csv\n",
    "VALID_FILES=gs://${BUCKET}/data/babyweight/big_data/eval-*.csv\n",
    "MODEL_DIR=gs://${BUCKET}/models/babyweight/${MODEL_NAME}_tune\n",
    "\n",
    "CURRENT_DATE=`date +%Y%m%d_%H%M%S`\n",
    "JOB_NAME=tune_${MODEL_NAME}_${TIER}_${CURRENT_DATE}\n",
    "\n",
    "gcloud ml-engine jobs submit training ${JOB_NAME} \\\n",
    "        --job-dir=${MODEL_DIR} \\\n",
    "        --runtime-version=1.4 \\\n",
    "        --region=${REGION} \\\n",
    "        --module-name=trainer.task \\\n",
    "        --package-path=${PACKAGE_PATH} \\\n",
    "        --config=ml-packages/babyweight-tf1.4/hyperparams.yaml \\\n",
    "        -- \\\n",
    "        --train-files=${TRAIN_FILES} \\\n",
    "        --num-epochs=100 \\\n",
    "        --train-batch-size=1000 \\\n",
    "        --eval-files=${VALID_FILES} \\\n",
    "        --eval-batch-size=1000 \\\n",
    "        --job-dir=${MODEL_DIR}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Deploy the Model on Cloud ML Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "REGION=europe-west1\n",
    "BUCKET=ksalama-gcs-cloudml\n",
    "\n",
    "MODEL_NAME=\"babyweight_estimator\"\n",
    "MODEL_VERSION=\"v1\"\n",
    "\n",
    "MODEL_BINARIES=$(gsutil ls gs://${BUCKET}/models/babyweight/${MODEL_NAME}/export/estimate | tail -1)\n",
    "\n",
    "gsutil ls ${MODEL_BINARIES}\n",
    "\n",
    "# #delete model version\n",
    "# gcloud ml-engine versions delete ${MODEL_VERSION} --model=${MODEL_NAME}\n",
    "\n",
    "# #delete model\n",
    "# gcloud ml-engine models delete ${MODEL_NAME}\n",
    "\n",
    "# #deploy model to GCP\n",
    "# gcloud ml-engine models create ${MODEL_NAME} --regions=${REGION}\n",
    "\n",
    "# #deploy model version\n",
    "# gcloud ml-engine versions create ${MODEL_VERSION} --model=${MODEL_NAME} --origin=${MODEL_BINARIES} --runtime-version=1.4\n",
    "\n",
    "# echo  ${MODEL_NAME} ${MODEL_VERSION} \n",
    "# #invoke deployed model to make prediction given new data instances\n",
    "# gcloud ml-engine predict --model=${MODEL_NAME} --version=${MODEL_VERSION} --json-instances=data/babyweight/new-data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Consume the Depoyed Model as API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googleapiclient import discovery\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "def estimate(project, model_name, version, instances):\n",
    "\n",
    "    credentials = GoogleCredentials.get_application_default()\n",
    "    api = discovery.build('ml', 'v1', credentials=credentials,\n",
    "                discoveryServiceUrl='https://storage.googleapis.com/cloud-ml/discovery/ml_v1_discovery.json')\n",
    "\n",
    "    request_data = {'instances': instances}\n",
    "\n",
    "    model_url = 'projects/{}/models/{}/versions/{}'.format(project, model_name, version)\n",
    "    response = api.projects().predict(body=request_data, name=model_url).execute()\n",
    "\n",
    "    estimates = list(map(lambda item: round(item[\"scores\"],2)\n",
    "        ,response[\"predictions\"]\n",
    "    ))\n",
    "\n",
    "    return estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT='ksalama-gcp-playground'\n",
    "MODEL_NAME='babyweight_estimator'\n",
    "VERSION='v1'\n",
    "\n",
    "instances = [\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'False',\n",
    "        'mother_age': 29.0,\n",
    "        'mother_race': 'Asian Indian',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 38,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'White',\n",
    "        'plurality': 1.0,\n",
    "        'gestation_weeks': 39,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'False'\n",
    "      },\n",
    "      {\n",
    "        'is_male': 'True',\n",
    "        'mother_age': 26.0,\n",
    "        'mother_race': 'White',\n",
    "        'plurality': 2.0,\n",
    "        'gestation_weeks': 37,\n",
    "        'mother_married': 'True',\n",
    "        'cigarette_use': 'False',\n",
    "        'alcohol_use': 'True'\n",
    "      }\n",
    "  ]\n",
    "\n",
    "estimates = estimate(instances=instances\n",
    "                     ,project=PROJECT\n",
    "                     ,model_name=MODEL_NAME\n",
    "                     ,version=VERSION)\n",
    "\n",
    "print(estimates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
